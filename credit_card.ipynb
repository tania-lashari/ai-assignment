{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    },
    "colab": {
      "name": "credit_card_0404_final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3DaR19Ts_hj",
        "outputId": "65e12c74-b448-444b-b608-e5b47a2fb963"
      },
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "\n",
        "# Get the real data from https://www.kaggle.com/mlg-ulb/creditcardfraud/\n",
        "fname = \"/content/drive/MyDrive/PIAIC/credit_card/creditcard.csv\"\n",
        "\n",
        "all_features = []\n",
        "all_targets = []\n",
        "with open(fname) as f:\n",
        "    for i, line in enumerate(f):\n",
        "        if i == 0:\n",
        "            print(\"HEADER:\", line.strip())\n",
        "            continue  # Skip header\n",
        "        fields = line.strip().split(\",\")\n",
        "        all_features.append([float(v.replace('\"', \"\")) for v in fields[:-1]])\n",
        "        all_targets.append([int(fields[-1].replace('\"', \"\"))])\n",
        "        if i == 1:\n",
        "            print(\"EXAMPLE FEATURES:\", all_features[-1])\n",
        "\n",
        "features = np.array(all_features, dtype=\"float32\")\n",
        "targets = np.array(all_targets, dtype=\"uint8\")\n",
        "print(\"features.shape:\", features.shape)\n",
        "print(\"targets.shape:\", targets.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "HEADER: \"Time\",\"V1\",\"V2\",\"V3\",\"V4\",\"V5\",\"V6\",\"V7\",\"V8\",\"V9\",\"V10\",\"V11\",\"V12\",\"V13\",\"V14\",\"V15\",\"V16\",\"V17\",\"V18\",\"V19\",\"V20\",\"V21\",\"V22\",\"V23\",\"V24\",\"V25\",\"V26\",\"V27\",\"V28\",\"Amount\",\"Class\"\n",
            "EXAMPLE FEATURES: [0.0, -1.3598071336738, -0.0727811733098497, 2.53634673796914, 1.37815522427443, -0.338320769942518, 0.462387777762292, 0.239598554061257, 0.0986979012610507, 0.363786969611213, 0.0907941719789316, -0.551599533260813, -0.617800855762348, -0.991389847235408, -0.311169353699879, 1.46817697209427, -0.470400525259478, 0.207971241929242, 0.0257905801985591, 0.403992960255733, 0.251412098239705, -0.018306777944153, 0.277837575558899, -0.110473910188767, 0.0669280749146731, 0.128539358273528, -0.189114843888824, 0.133558376740387, -0.0210530534538215, 149.62]\n",
            "features.shape: (284807, 30)\n",
            "targets.shape: (284807, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68e2qQ_5tFjP"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YeG3E9T6s_hu",
        "outputId": "99ffda96-162a-4d39-932b-56bf1a1e886b"
      },
      "source": [
        "num_val_samples = int(len(features) * 0.2)\n",
        "train_features = features[:-num_val_samples]\n",
        "train_targets = targets[:-num_val_samples]\n",
        "val_features = features[-num_val_samples:]\n",
        "val_targets = targets[-num_val_samples:]\n",
        "\n",
        "print(\"Number of training samples:\", len(train_features))\n",
        "print(\"Number of validation samples:\", len(val_features))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training samples: 227846\n",
            "Number of validation samples: 56961\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7h5a8Ews_hv",
        "outputId": "c95ed397-cda2-4482-e367-712d8d7ea84f"
      },
      "source": [
        "counts = np.bincount(train_targets[:, 0])\n",
        "print(\n",
        "    \"Number of positive samples in training data: {} ({:.2f}% of total)\".format(\n",
        "        counts[1], 100 * float(counts[1]) / len(train_targets)\n",
        "    )\n",
        ")\n",
        "\n",
        "weight_for_0 = 1.0 / counts[0]\n",
        "weight_for_1 = 1.0 / counts[1]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of positive samples in training data: 417 (0.18% of total)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rvqbsk7rs_hv",
        "outputId": "fb589a2b-34f9-43d1-f6c0-64e4d8ba718a"
      },
      "source": [
        "weight_for_0"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.396976638863118e-06"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSwoXm7Cs_hw",
        "outputId": "462de7e3-d9ec-4a94-9482-9ba54ebe88ec"
      },
      "source": [
        "weight_for_1"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.002398081534772182"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htnQTFANs_hw"
      },
      "source": [
        "mean = np.mean(train_features, axis=0)\n",
        "train_features -= mean\n",
        "val_features -= mean\n",
        "std = np.std(train_features, axis=0)\n",
        "train_features /= std\n",
        "val_features /= std "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_7xnVVEs_hw",
        "outputId": "80013169-90f5-4134-ad02-165c05b335e5"
      },
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.layers.Dense(\n",
        "            256, activation=\"relu\", input_shape=(train_features.shape[-1],)\n",
        "        ),\n",
        "        keras.layers.Dense(256, activation=\"relu\"),\n",
        "        keras.layers.Dropout(0.3),\n",
        "        keras.layers.Dense(256, activation=\"relu\"),\n",
        "        keras.layers.Dropout(0.3),\n",
        "        keras.layers.Dense(1, activation=\"sigmoid\"),\n",
        "    ]\n",
        ")\n",
        "model.summary()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_4 (Dense)              (None, 256)               7936      \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 139,777\n",
            "Trainable params: 139,777\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdNhT19at8ws"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgJQ0zyRs_hx",
        "outputId": "dea55927-12c8-477a-b2f1-3176732d8e67"
      },
      "source": [
        "with tf.device('/device:GPU:0'):\n",
        "  metrics = [\n",
        "      keras.metrics.FalseNegatives(name=\"fn\"),\n",
        "      keras.metrics.FalsePositives(name=\"fp\"),\n",
        "      keras.metrics.TrueNegatives(name=\"tn\"),\n",
        "      keras.metrics.TruePositives(name=\"tp\"),\n",
        "      keras.metrics.Precision(name=\"precision\"),\n",
        "      keras.metrics.Recall(name=\"recall\"),\n",
        "  ]\n",
        "\n",
        "  model.compile(\n",
        "      optimizer=keras.optimizers.Adam(1e-2), loss=\"binary_crossentropy\", metrics=metrics\n",
        "  )\n",
        "\n",
        "  callbacks = [keras.callbacks.ModelCheckpoint(\"fraud_model_at_epoch_{epoch}.h5\")]\n",
        "  class_weight = {0: weight_for_0, 1: weight_for_1}\n",
        "\n",
        "  model.fit(\n",
        "      train_features,\n",
        "      train_targets,\n",
        "      batch_size=2048,\n",
        "      epochs=100,\n",
        "      verbose=2,\n",
        "      callbacks=callbacks,\n",
        "      validation_data=(val_features, val_targets),\n",
        "      class_weight=class_weight,\n",
        "  )"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "112/112 - 2s - loss: 2.2255e-06 - fn: 51.0000 - fp: 19344.0000 - tn: 208085.0000 - tp: 366.0000 - precision: 0.0186 - recall: 0.8777 - val_loss: 0.3441 - val_fn: 2.0000 - val_fp: 7111.0000 - val_tn: 49775.0000 - val_tp: 73.0000 - val_precision: 0.0102 - val_recall: 0.9733\n",
            "Epoch 2/100\n",
            "112/112 - 1s - loss: 1.3727e-06 - fn: 29.0000 - fp: 9266.0000 - tn: 218163.0000 - tp: 388.0000 - precision: 0.0402 - recall: 0.9305 - val_loss: 0.0362 - val_fn: 13.0000 - val_fp: 271.0000 - val_tn: 56615.0000 - val_tp: 62.0000 - val_precision: 0.1862 - val_recall: 0.8267\n",
            "Epoch 3/100\n",
            "112/112 - 1s - loss: 1.0529e-06 - fn: 26.0000 - fp: 5882.0000 - tn: 221547.0000 - tp: 391.0000 - precision: 0.0623 - recall: 0.9376 - val_loss: 0.0440 - val_fn: 10.0000 - val_fp: 385.0000 - val_tn: 56501.0000 - val_tp: 65.0000 - val_precision: 0.1444 - val_recall: 0.8667\n",
            "Epoch 4/100\n",
            "112/112 - 1s - loss: 1.1788e-06 - fn: 26.0000 - fp: 8442.0000 - tn: 218987.0000 - tp: 391.0000 - precision: 0.0443 - recall: 0.9376 - val_loss: 0.0582 - val_fn: 10.0000 - val_fp: 697.0000 - val_tn: 56189.0000 - val_tp: 65.0000 - val_precision: 0.0853 - val_recall: 0.8667\n",
            "Epoch 5/100\n",
            "112/112 - 1s - loss: 9.7387e-07 - fn: 25.0000 - fp: 6797.0000 - tn: 220632.0000 - tp: 392.0000 - precision: 0.0545 - recall: 0.9400 - val_loss: 0.1016 - val_fn: 7.0000 - val_fp: 2098.0000 - val_tn: 54788.0000 - val_tp: 68.0000 - val_precision: 0.0314 - val_recall: 0.9067\n",
            "Epoch 6/100\n",
            "112/112 - 1s - loss: 6.6213e-07 - fn: 16.0000 - fp: 5385.0000 - tn: 222044.0000 - tp: 401.0000 - precision: 0.0693 - recall: 0.9616 - val_loss: 0.0591 - val_fn: 9.0000 - val_fp: 1117.0000 - val_tn: 55769.0000 - val_tp: 66.0000 - val_precision: 0.0558 - val_recall: 0.8800\n",
            "Epoch 7/100\n",
            "112/112 - 1s - loss: 7.6626e-07 - fn: 13.0000 - fp: 6082.0000 - tn: 221347.0000 - tp: 404.0000 - precision: 0.0623 - recall: 0.9688 - val_loss: 0.0409 - val_fn: 8.0000 - val_fp: 690.0000 - val_tn: 56196.0000 - val_tp: 67.0000 - val_precision: 0.0885 - val_recall: 0.8933\n",
            "Epoch 8/100\n",
            "112/112 - 1s - loss: 7.9714e-07 - fn: 14.0000 - fp: 8763.0000 - tn: 218666.0000 - tp: 403.0000 - precision: 0.0440 - recall: 0.9664 - val_loss: 0.0362 - val_fn: 13.0000 - val_fp: 575.0000 - val_tn: 56311.0000 - val_tp: 62.0000 - val_precision: 0.0973 - val_recall: 0.8267\n",
            "Epoch 9/100\n",
            "112/112 - 1s - loss: 6.3874e-07 - fn: 17.0000 - fp: 6730.0000 - tn: 220699.0000 - tp: 400.0000 - precision: 0.0561 - recall: 0.9592 - val_loss: 0.0200 - val_fn: 10.0000 - val_fp: 398.0000 - val_tn: 56488.0000 - val_tp: 65.0000 - val_precision: 0.1404 - val_recall: 0.8667\n",
            "Epoch 10/100\n",
            "112/112 - 1s - loss: 6.1709e-07 - fn: 10.0000 - fp: 6210.0000 - tn: 221219.0000 - tp: 407.0000 - precision: 0.0615 - recall: 0.9760 - val_loss: 0.0444 - val_fn: 7.0000 - val_fp: 981.0000 - val_tn: 55905.0000 - val_tp: 68.0000 - val_precision: 0.0648 - val_recall: 0.9067\n",
            "Epoch 11/100\n",
            "112/112 - 1s - loss: 7.2192e-07 - fn: 9.0000 - fp: 8600.0000 - tn: 218829.0000 - tp: 408.0000 - precision: 0.0453 - recall: 0.9784 - val_loss: 0.0457 - val_fn: 10.0000 - val_fp: 839.0000 - val_tn: 56047.0000 - val_tp: 65.0000 - val_precision: 0.0719 - val_recall: 0.8667\n",
            "Epoch 12/100\n",
            "112/112 - 1s - loss: 5.8048e-07 - fn: 12.0000 - fp: 5142.0000 - tn: 222287.0000 - tp: 405.0000 - precision: 0.0730 - recall: 0.9712 - val_loss: 0.0557 - val_fn: 7.0000 - val_fp: 1028.0000 - val_tn: 55858.0000 - val_tp: 68.0000 - val_precision: 0.0620 - val_recall: 0.9067\n",
            "Epoch 13/100\n",
            "112/112 - 1s - loss: 4.7849e-07 - fn: 7.0000 - fp: 5346.0000 - tn: 222083.0000 - tp: 410.0000 - precision: 0.0712 - recall: 0.9832 - val_loss: 0.0314 - val_fn: 9.0000 - val_fp: 728.0000 - val_tn: 56158.0000 - val_tp: 66.0000 - val_precision: 0.0831 - val_recall: 0.8800\n",
            "Epoch 14/100\n",
            "112/112 - 1s - loss: 6.2836e-07 - fn: 5.0000 - fp: 5688.0000 - tn: 221741.0000 - tp: 412.0000 - precision: 0.0675 - recall: 0.9880 - val_loss: 0.3190 - val_fn: 8.0000 - val_fp: 1709.0000 - val_tn: 55177.0000 - val_tp: 67.0000 - val_precision: 0.0377 - val_recall: 0.8933\n",
            "Epoch 15/100\n",
            "112/112 - 1s - loss: 2.8221e-06 - fn: 25.0000 - fp: 8458.0000 - tn: 218971.0000 - tp: 392.0000 - precision: 0.0443 - recall: 0.9400 - val_loss: 0.2134 - val_fn: 7.0000 - val_fp: 2171.0000 - val_tn: 54715.0000 - val_tp: 68.0000 - val_precision: 0.0304 - val_recall: 0.9067\n",
            "Epoch 16/100\n",
            "112/112 - 1s - loss: 1.4176e-06 - fn: 15.0000 - fp: 6668.0000 - tn: 220761.0000 - tp: 402.0000 - precision: 0.0569 - recall: 0.9640 - val_loss: 0.2287 - val_fn: 9.0000 - val_fp: 2106.0000 - val_tn: 54780.0000 - val_tp: 66.0000 - val_precision: 0.0304 - val_recall: 0.8800\n",
            "Epoch 17/100\n",
            "112/112 - 1s - loss: 3.3645e-06 - fn: 16.0000 - fp: 5580.0000 - tn: 221849.0000 - tp: 401.0000 - precision: 0.0670 - recall: 0.9616 - val_loss: 0.2648 - val_fn: 9.0000 - val_fp: 1230.0000 - val_tn: 55656.0000 - val_tp: 66.0000 - val_precision: 0.0509 - val_recall: 0.8800\n",
            "Epoch 18/100\n",
            "112/112 - 1s - loss: 3.7345e-06 - fn: 10.0000 - fp: 7176.0000 - tn: 220253.0000 - tp: 407.0000 - precision: 0.0537 - recall: 0.9760 - val_loss: 0.1163 - val_fn: 7.0000 - val_fp: 2194.0000 - val_tn: 54692.0000 - val_tp: 68.0000 - val_precision: 0.0301 - val_recall: 0.9067\n",
            "Epoch 19/100\n",
            "112/112 - 1s - loss: 3.5190e-06 - fn: 12.0000 - fp: 6841.0000 - tn: 220588.0000 - tp: 405.0000 - precision: 0.0559 - recall: 0.9712 - val_loss: 0.2006 - val_fn: 9.0000 - val_fp: 1020.0000 - val_tn: 55866.0000 - val_tp: 66.0000 - val_precision: 0.0608 - val_recall: 0.8800\n",
            "Epoch 20/100\n",
            "112/112 - 1s - loss: 2.0583e-06 - fn: 13.0000 - fp: 6547.0000 - tn: 220882.0000 - tp: 404.0000 - precision: 0.0581 - recall: 0.9688 - val_loss: 0.0658 - val_fn: 12.0000 - val_fp: 339.0000 - val_tn: 56547.0000 - val_tp: 63.0000 - val_precision: 0.1567 - val_recall: 0.8400\n",
            "Epoch 21/100\n",
            "112/112 - 1s - loss: 7.5627e-07 - fn: 12.0000 - fp: 4096.0000 - tn: 223333.0000 - tp: 405.0000 - precision: 0.0900 - recall: 0.9712 - val_loss: 0.0443 - val_fn: 9.0000 - val_fp: 506.0000 - val_tn: 56380.0000 - val_tp: 66.0000 - val_precision: 0.1154 - val_recall: 0.8800\n",
            "Epoch 22/100\n",
            "112/112 - 1s - loss: 6.6298e-07 - fn: 9.0000 - fp: 3681.0000 - tn: 223748.0000 - tp: 408.0000 - precision: 0.0998 - recall: 0.9784 - val_loss: 0.0438 - val_fn: 8.0000 - val_fp: 767.0000 - val_tn: 56119.0000 - val_tp: 67.0000 - val_precision: 0.0803 - val_recall: 0.8933\n",
            "Epoch 23/100\n",
            "112/112 - 1s - loss: 3.2356e-07 - fn: 5.0000 - fp: 2787.0000 - tn: 224642.0000 - tp: 412.0000 - precision: 0.1288 - recall: 0.9880 - val_loss: 0.0255 - val_fn: 9.0000 - val_fp: 313.0000 - val_tn: 56573.0000 - val_tp: 66.0000 - val_precision: 0.1741 - val_recall: 0.8800\n",
            "Epoch 24/100\n",
            "112/112 - 1s - loss: 2.8442e-07 - fn: 4.0000 - fp: 2446.0000 - tn: 224983.0000 - tp: 413.0000 - precision: 0.1445 - recall: 0.9904 - val_loss: 0.0214 - val_fn: 11.0000 - val_fp: 351.0000 - val_tn: 56535.0000 - val_tp: 64.0000 - val_precision: 0.1542 - val_recall: 0.8533\n",
            "Epoch 25/100\n",
            "112/112 - 1s - loss: 2.1495e-07 - fn: 2.0000 - fp: 1788.0000 - tn: 225641.0000 - tp: 415.0000 - precision: 0.1884 - recall: 0.9952 - val_loss: 0.0180 - val_fn: 9.0000 - val_fp: 297.0000 - val_tn: 56589.0000 - val_tp: 66.0000 - val_precision: 0.1818 - val_recall: 0.8800\n",
            "Epoch 26/100\n",
            "112/112 - 1s - loss: 3.2628e-07 - fn: 5.0000 - fp: 2722.0000 - tn: 224707.0000 - tp: 412.0000 - precision: 0.1315 - recall: 0.9880 - val_loss: 0.1859 - val_fn: 11.0000 - val_fp: 995.0000 - val_tn: 55891.0000 - val_tp: 64.0000 - val_precision: 0.0604 - val_recall: 0.8533\n",
            "Epoch 27/100\n",
            "112/112 - 1s - loss: 6.4138e-07 - fn: 8.0000 - fp: 4594.0000 - tn: 222835.0000 - tp: 409.0000 - precision: 0.0818 - recall: 0.9808 - val_loss: 0.0296 - val_fn: 9.0000 - val_fp: 610.0000 - val_tn: 56276.0000 - val_tp: 66.0000 - val_precision: 0.0976 - val_recall: 0.8800\n",
            "Epoch 28/100\n",
            "112/112 - 1s - loss: 3.7933e-07 - fn: 6.0000 - fp: 3808.0000 - tn: 223621.0000 - tp: 411.0000 - precision: 0.0974 - recall: 0.9856 - val_loss: 0.0131 - val_fn: 11.0000 - val_fp: 212.0000 - val_tn: 56674.0000 - val_tp: 64.0000 - val_precision: 0.2319 - val_recall: 0.8533\n",
            "Epoch 29/100\n",
            "112/112 - 1s - loss: 6.3901e-07 - fn: 5.0000 - fp: 3893.0000 - tn: 223536.0000 - tp: 412.0000 - precision: 0.0957 - recall: 0.9880 - val_loss: 0.0504 - val_fn: 8.0000 - val_fp: 907.0000 - val_tn: 55979.0000 - val_tp: 67.0000 - val_precision: 0.0688 - val_recall: 0.8933\n",
            "Epoch 30/100\n",
            "112/112 - 1s - loss: 5.2848e-07 - fn: 4.0000 - fp: 5338.0000 - tn: 222091.0000 - tp: 413.0000 - precision: 0.0718 - recall: 0.9904 - val_loss: 0.0336 - val_fn: 9.0000 - val_fp: 599.0000 - val_tn: 56287.0000 - val_tp: 66.0000 - val_precision: 0.0992 - val_recall: 0.8800\n",
            "Epoch 31/100\n",
            "112/112 - 1s - loss: 2.9597e-07 - fn: 5.0000 - fp: 2812.0000 - tn: 224617.0000 - tp: 412.0000 - precision: 0.1278 - recall: 0.9880 - val_loss: 0.0231 - val_fn: 9.0000 - val_fp: 405.0000 - val_tn: 56481.0000 - val_tp: 66.0000 - val_precision: 0.1401 - val_recall: 0.8800\n",
            "Epoch 32/100\n",
            "112/112 - 1s - loss: 3.9741e-07 - fn: 2.0000 - fp: 2227.0000 - tn: 225202.0000 - tp: 415.0000 - precision: 0.1571 - recall: 0.9952 - val_loss: 0.0129 - val_fn: 11.0000 - val_fp: 167.0000 - val_tn: 56719.0000 - val_tp: 64.0000 - val_precision: 0.2771 - val_recall: 0.8533\n",
            "Epoch 33/100\n",
            "112/112 - 1s - loss: 3.4627e-07 - fn: 4.0000 - fp: 3376.0000 - tn: 224053.0000 - tp: 413.0000 - precision: 0.1090 - recall: 0.9904 - val_loss: 0.0141 - val_fn: 11.0000 - val_fp: 227.0000 - val_tn: 56659.0000 - val_tp: 64.0000 - val_precision: 0.2199 - val_recall: 0.8533\n",
            "Epoch 34/100\n",
            "112/112 - 1s - loss: 3.8626e-07 - fn: 4.0000 - fp: 2284.0000 - tn: 225145.0000 - tp: 413.0000 - precision: 0.1531 - recall: 0.9904 - val_loss: 0.0104 - val_fn: 13.0000 - val_fp: 147.0000 - val_tn: 56739.0000 - val_tp: 62.0000 - val_precision: 0.2967 - val_recall: 0.8267\n",
            "Epoch 35/100\n",
            "112/112 - 1s - loss: 5.9020e-07 - fn: 3.0000 - fp: 3554.0000 - tn: 223875.0000 - tp: 414.0000 - precision: 0.1043 - recall: 0.9928 - val_loss: 0.0128 - val_fn: 10.0000 - val_fp: 222.0000 - val_tn: 56664.0000 - val_tp: 65.0000 - val_precision: 0.2265 - val_recall: 0.8667\n",
            "Epoch 36/100\n",
            "112/112 - 1s - loss: 2.8345e-07 - fn: 3.0000 - fp: 2506.0000 - tn: 224923.0000 - tp: 414.0000 - precision: 0.1418 - recall: 0.9928 - val_loss: 0.0185 - val_fn: 11.0000 - val_fp: 340.0000 - val_tn: 56546.0000 - val_tp: 64.0000 - val_precision: 0.1584 - val_recall: 0.8533\n",
            "Epoch 37/100\n",
            "112/112 - 1s - loss: 1.6225e-07 - fn: 0.0000e+00 - fp: 1736.0000 - tn: 225693.0000 - tp: 417.0000 - precision: 0.1937 - recall: 1.0000 - val_loss: 0.0106 - val_fn: 11.0000 - val_fp: 131.0000 - val_tn: 56755.0000 - val_tp: 64.0000 - val_precision: 0.3282 - val_recall: 0.8533\n",
            "Epoch 38/100\n",
            "112/112 - 1s - loss: 1.7824e-07 - fn: 2.0000 - fp: 1683.0000 - tn: 225746.0000 - tp: 415.0000 - precision: 0.1978 - recall: 0.9952 - val_loss: 0.0113 - val_fn: 16.0000 - val_fp: 120.0000 - val_tn: 56766.0000 - val_tp: 59.0000 - val_precision: 0.3296 - val_recall: 0.7867\n",
            "Epoch 39/100\n",
            "112/112 - 1s - loss: 2.0713e-07 - fn: 2.0000 - fp: 1712.0000 - tn: 225717.0000 - tp: 415.0000 - precision: 0.1951 - recall: 0.9952 - val_loss: 0.0837 - val_fn: 12.0000 - val_fp: 470.0000 - val_tn: 56416.0000 - val_tp: 63.0000 - val_precision: 0.1182 - val_recall: 0.8400\n",
            "Epoch 40/100\n",
            "112/112 - 1s - loss: 3.4924e-07 - fn: 2.0000 - fp: 2844.0000 - tn: 224585.0000 - tp: 415.0000 - precision: 0.1273 - recall: 0.9952 - val_loss: 0.0454 - val_fn: 10.0000 - val_fp: 1081.0000 - val_tn: 55805.0000 - val_tp: 65.0000 - val_precision: 0.0567 - val_recall: 0.8667\n",
            "Epoch 41/100\n",
            "112/112 - 1s - loss: 2.5944e-07 - fn: 3.0000 - fp: 2748.0000 - tn: 224681.0000 - tp: 414.0000 - precision: 0.1309 - recall: 0.9928 - val_loss: 0.0108 - val_fn: 14.0000 - val_fp: 106.0000 - val_tn: 56780.0000 - val_tp: 61.0000 - val_precision: 0.3653 - val_recall: 0.8133\n",
            "Epoch 42/100\n",
            "112/112 - 1s - loss: 2.7486e-07 - fn: 1.0000 - fp: 1380.0000 - tn: 226049.0000 - tp: 416.0000 - precision: 0.2316 - recall: 0.9976 - val_loss: 0.0208 - val_fn: 12.0000 - val_fp: 229.0000 - val_tn: 56657.0000 - val_tp: 63.0000 - val_precision: 0.2158 - val_recall: 0.8400\n",
            "Epoch 43/100\n",
            "112/112 - 1s - loss: 3.1611e-07 - fn: 3.0000 - fp: 1606.0000 - tn: 225823.0000 - tp: 414.0000 - precision: 0.2050 - recall: 0.9928 - val_loss: 0.0222 - val_fn: 10.0000 - val_fp: 382.0000 - val_tn: 56504.0000 - val_tp: 65.0000 - val_precision: 0.1454 - val_recall: 0.8667\n",
            "Epoch 44/100\n",
            "112/112 - 1s - loss: 1.9506e-07 - fn: 1.0000 - fp: 2046.0000 - tn: 225383.0000 - tp: 416.0000 - precision: 0.1690 - recall: 0.9976 - val_loss: 0.0372 - val_fn: 8.0000 - val_fp: 548.0000 - val_tn: 56338.0000 - val_tp: 67.0000 - val_precision: 0.1089 - val_recall: 0.8933\n",
            "Epoch 45/100\n",
            "112/112 - 1s - loss: 1.9964e-07 - fn: 1.0000 - fp: 1987.0000 - tn: 225442.0000 - tp: 416.0000 - precision: 0.1731 - recall: 0.9976 - val_loss: 0.0126 - val_fn: 11.0000 - val_fp: 173.0000 - val_tn: 56713.0000 - val_tp: 64.0000 - val_precision: 0.2700 - val_recall: 0.8533\n",
            "Epoch 46/100\n",
            "112/112 - 1s - loss: 3.6496e-07 - fn: 4.0000 - fp: 3536.0000 - tn: 223893.0000 - tp: 413.0000 - precision: 0.1046 - recall: 0.9904 - val_loss: 0.0135 - val_fn: 12.0000 - val_fp: 140.0000 - val_tn: 56746.0000 - val_tp: 63.0000 - val_precision: 0.3103 - val_recall: 0.8400\n",
            "Epoch 47/100\n",
            "112/112 - 1s - loss: 6.4524e-07 - fn: 5.0000 - fp: 4568.0000 - tn: 222861.0000 - tp: 412.0000 - precision: 0.0827 - recall: 0.9880 - val_loss: 0.0284 - val_fn: 9.0000 - val_fp: 505.0000 - val_tn: 56381.0000 - val_tp: 66.0000 - val_precision: 0.1156 - val_recall: 0.8800\n",
            "Epoch 48/100\n",
            "112/112 - 1s - loss: 2.8712e-07 - fn: 6.0000 - fp: 2714.0000 - tn: 224715.0000 - tp: 411.0000 - precision: 0.1315 - recall: 0.9856 - val_loss: 0.0148 - val_fn: 10.0000 - val_fp: 205.0000 - val_tn: 56681.0000 - val_tp: 65.0000 - val_precision: 0.2407 - val_recall: 0.8667\n",
            "Epoch 49/100\n",
            "112/112 - 1s - loss: 1.8673e-07 - fn: 3.0000 - fp: 1829.0000 - tn: 225600.0000 - tp: 414.0000 - precision: 0.1846 - recall: 0.9928 - val_loss: 0.0256 - val_fn: 9.0000 - val_fp: 512.0000 - val_tn: 56374.0000 - val_tp: 66.0000 - val_precision: 0.1142 - val_recall: 0.8800\n",
            "Epoch 50/100\n",
            "112/112 - 1s - loss: 1.4914e-07 - fn: 1.0000 - fp: 1801.0000 - tn: 225628.0000 - tp: 416.0000 - precision: 0.1876 - recall: 0.9976 - val_loss: 0.0101 - val_fn: 11.0000 - val_fp: 117.0000 - val_tn: 56769.0000 - val_tp: 64.0000 - val_precision: 0.3536 - val_recall: 0.8533\n",
            "Epoch 51/100\n",
            "112/112 - 1s - loss: 1.1231e-07 - fn: 0.0000e+00 - fp: 1086.0000 - tn: 226343.0000 - tp: 417.0000 - precision: 0.2774 - recall: 1.0000 - val_loss: 0.0107 - val_fn: 10.0000 - val_fp: 145.0000 - val_tn: 56741.0000 - val_tp: 65.0000 - val_precision: 0.3095 - val_recall: 0.8667\n",
            "Epoch 52/100\n",
            "112/112 - 1s - loss: 1.1736e-07 - fn: 1.0000 - fp: 1211.0000 - tn: 226218.0000 - tp: 416.0000 - precision: 0.2557 - recall: 0.9976 - val_loss: 0.0093 - val_fn: 11.0000 - val_fp: 83.0000 - val_tn: 56803.0000 - val_tp: 64.0000 - val_precision: 0.4354 - val_recall: 0.8533\n",
            "Epoch 53/100\n",
            "112/112 - 1s - loss: 3.7696e-07 - fn: 4.0000 - fp: 2419.0000 - tn: 225010.0000 - tp: 413.0000 - precision: 0.1458 - recall: 0.9904 - val_loss: 0.0103 - val_fn: 9.0000 - val_fp: 140.0000 - val_tn: 56746.0000 - val_tp: 66.0000 - val_precision: 0.3204 - val_recall: 0.8800\n",
            "Epoch 54/100\n",
            "112/112 - 1s - loss: 1.8640e-07 - fn: 1.0000 - fp: 2122.0000 - tn: 225307.0000 - tp: 416.0000 - precision: 0.1639 - recall: 0.9976 - val_loss: 0.0107 - val_fn: 9.0000 - val_fp: 150.0000 - val_tn: 56736.0000 - val_tp: 66.0000 - val_precision: 0.3056 - val_recall: 0.8800\n",
            "Epoch 55/100\n",
            "112/112 - 1s - loss: 1.3532e-07 - fn: 1.0000 - fp: 1459.0000 - tn: 225970.0000 - tp: 416.0000 - precision: 0.2219 - recall: 0.9976 - val_loss: 0.0111 - val_fn: 10.0000 - val_fp: 124.0000 - val_tn: 56762.0000 - val_tp: 65.0000 - val_precision: 0.3439 - val_recall: 0.8667\n",
            "Epoch 56/100\n",
            "112/112 - 1s - loss: 1.9783e-07 - fn: 1.0000 - fp: 2243.0000 - tn: 225186.0000 - tp: 416.0000 - precision: 0.1564 - recall: 0.9976 - val_loss: 0.0107 - val_fn: 12.0000 - val_fp: 128.0000 - val_tn: 56758.0000 - val_tp: 63.0000 - val_precision: 0.3298 - val_recall: 0.8400\n",
            "Epoch 57/100\n",
            "112/112 - 1s - loss: 3.5736e-07 - fn: 4.0000 - fp: 2773.0000 - tn: 224656.0000 - tp: 413.0000 - precision: 0.1296 - recall: 0.9904 - val_loss: 0.0728 - val_fn: 6.0000 - val_fp: 1281.0000 - val_tn: 55605.0000 - val_tp: 69.0000 - val_precision: 0.0511 - val_recall: 0.9200\n",
            "Epoch 58/100\n",
            "112/112 - 1s - loss: 3.7037e-07 - fn: 4.0000 - fp: 4119.0000 - tn: 223310.0000 - tp: 413.0000 - precision: 0.0911 - recall: 0.9904 - val_loss: 0.0140 - val_fn: 9.0000 - val_fp: 287.0000 - val_tn: 56599.0000 - val_tp: 66.0000 - val_precision: 0.1870 - val_recall: 0.8800\n",
            "Epoch 59/100\n",
            "112/112 - 1s - loss: 2.6320e-07 - fn: 3.0000 - fp: 2502.0000 - tn: 224927.0000 - tp: 414.0000 - precision: 0.1420 - recall: 0.9928 - val_loss: 0.0127 - val_fn: 9.0000 - val_fp: 193.0000 - val_tn: 56693.0000 - val_tp: 66.0000 - val_precision: 0.2548 - val_recall: 0.8800\n",
            "Epoch 60/100\n",
            "112/112 - 1s - loss: 2.1659e-07 - fn: 1.0000 - fp: 2761.0000 - tn: 224668.0000 - tp: 416.0000 - precision: 0.1309 - recall: 0.9976 - val_loss: 0.0184 - val_fn: 9.0000 - val_fp: 342.0000 - val_tn: 56544.0000 - val_tp: 66.0000 - val_precision: 0.1618 - val_recall: 0.8800\n",
            "Epoch 61/100\n",
            "112/112 - 1s - loss: 2.3924e-07 - fn: 2.0000 - fp: 2080.0000 - tn: 225349.0000 - tp: 415.0000 - precision: 0.1663 - recall: 0.9952 - val_loss: 0.0610 - val_fn: 9.0000 - val_fp: 241.0000 - val_tn: 56645.0000 - val_tp: 66.0000 - val_precision: 0.2150 - val_recall: 0.8800\n",
            "Epoch 62/100\n",
            "112/112 - 1s - loss: 3.7599e-07 - fn: 3.0000 - fp: 2265.0000 - tn: 225164.0000 - tp: 414.0000 - precision: 0.1545 - recall: 0.9928 - val_loss: 0.0260 - val_fn: 9.0000 - val_fp: 433.0000 - val_tn: 56453.0000 - val_tp: 66.0000 - val_precision: 0.1323 - val_recall: 0.8800\n",
            "Epoch 63/100\n",
            "112/112 - 1s - loss: 1.6976e-07 - fn: 1.0000 - fp: 1896.0000 - tn: 225533.0000 - tp: 416.0000 - precision: 0.1799 - recall: 0.9976 - val_loss: 0.0150 - val_fn: 10.0000 - val_fp: 245.0000 - val_tn: 56641.0000 - val_tp: 65.0000 - val_precision: 0.2097 - val_recall: 0.8667\n",
            "Epoch 64/100\n",
            "112/112 - 1s - loss: 1.1662e-07 - fn: 0.0000e+00 - fp: 1287.0000 - tn: 226142.0000 - tp: 417.0000 - precision: 0.2447 - recall: 1.0000 - val_loss: 0.0156 - val_fn: 9.0000 - val_fp: 194.0000 - val_tn: 56692.0000 - val_tp: 66.0000 - val_precision: 0.2538 - val_recall: 0.8800\n",
            "Epoch 65/100\n",
            "112/112 - 1s - loss: 1.0834e-07 - fn: 1.0000 - fp: 1038.0000 - tn: 226391.0000 - tp: 416.0000 - precision: 0.2861 - recall: 0.9976 - val_loss: 0.0284 - val_fn: 9.0000 - val_fp: 387.0000 - val_tn: 56499.0000 - val_tp: 66.0000 - val_precision: 0.1457 - val_recall: 0.8800\n",
            "Epoch 66/100\n",
            "112/112 - 1s - loss: 4.3450e-07 - fn: 3.0000 - fp: 3402.0000 - tn: 224027.0000 - tp: 414.0000 - precision: 0.1085 - recall: 0.9928 - val_loss: 0.0388 - val_fn: 10.0000 - val_fp: 672.0000 - val_tn: 56214.0000 - val_tp: 65.0000 - val_precision: 0.0882 - val_recall: 0.8667\n",
            "Epoch 67/100\n",
            "112/112 - 1s - loss: 2.0933e-07 - fn: 3.0000 - fp: 2273.0000 - tn: 225156.0000 - tp: 414.0000 - precision: 0.1541 - recall: 0.9928 - val_loss: 0.0151 - val_fn: 12.0000 - val_fp: 179.0000 - val_tn: 56707.0000 - val_tp: 63.0000 - val_precision: 0.2603 - val_recall: 0.8400\n",
            "Epoch 68/100\n",
            "112/112 - 1s - loss: 2.3254e-07 - fn: 3.0000 - fp: 2466.0000 - tn: 224963.0000 - tp: 414.0000 - precision: 0.1437 - recall: 0.9928 - val_loss: 0.0102 - val_fn: 12.0000 - val_fp: 121.0000 - val_tn: 56765.0000 - val_tp: 63.0000 - val_precision: 0.3424 - val_recall: 0.8400\n",
            "Epoch 69/100\n",
            "112/112 - 1s - loss: 2.4577e-07 - fn: 3.0000 - fp: 2730.0000 - tn: 224699.0000 - tp: 414.0000 - precision: 0.1317 - recall: 0.9928 - val_loss: 0.0152 - val_fn: 9.0000 - val_fp: 255.0000 - val_tn: 56631.0000 - val_tp: 66.0000 - val_precision: 0.2056 - val_recall: 0.8800\n",
            "Epoch 70/100\n",
            "112/112 - 1s - loss: 1.2923e-07 - fn: 1.0000 - fp: 1299.0000 - tn: 226130.0000 - tp: 416.0000 - precision: 0.2426 - recall: 0.9976 - val_loss: 0.0148 - val_fn: 12.0000 - val_fp: 201.0000 - val_tn: 56685.0000 - val_tp: 63.0000 - val_precision: 0.2386 - val_recall: 0.8400\n",
            "Epoch 71/100\n",
            "112/112 - 1s - loss: 2.5425e-07 - fn: 1.0000 - fp: 2077.0000 - tn: 225352.0000 - tp: 416.0000 - precision: 0.1669 - recall: 0.9976 - val_loss: 0.0110 - val_fn: 11.0000 - val_fp: 124.0000 - val_tn: 56762.0000 - val_tp: 64.0000 - val_precision: 0.3404 - val_recall: 0.8533\n",
            "Epoch 72/100\n",
            "112/112 - 1s - loss: 2.3442e-07 - fn: 2.0000 - fp: 2479.0000 - tn: 224950.0000 - tp: 415.0000 - precision: 0.1434 - recall: 0.9952 - val_loss: 0.0222 - val_fn: 9.0000 - val_fp: 507.0000 - val_tn: 56379.0000 - val_tp: 66.0000 - val_precision: 0.1152 - val_recall: 0.8800\n",
            "Epoch 73/100\n",
            "112/112 - 1s - loss: 1.1136e-07 - fn: 1.0000 - fp: 1483.0000 - tn: 225946.0000 - tp: 416.0000 - precision: 0.2191 - recall: 0.9976 - val_loss: 0.0102 - val_fn: 13.0000 - val_fp: 124.0000 - val_tn: 56762.0000 - val_tp: 62.0000 - val_precision: 0.3333 - val_recall: 0.8267\n",
            "Epoch 74/100\n",
            "112/112 - 1s - loss: 7.9954e-08 - fn: 0.0000e+00 - fp: 857.0000 - tn: 226572.0000 - tp: 417.0000 - precision: 0.3273 - recall: 1.0000 - val_loss: 0.0101 - val_fn: 13.0000 - val_fp: 95.0000 - val_tn: 56791.0000 - val_tp: 62.0000 - val_precision: 0.3949 - val_recall: 0.8267\n",
            "Epoch 75/100\n",
            "112/112 - 1s - loss: 1.2334e-07 - fn: 1.0000 - fp: 1079.0000 - tn: 226350.0000 - tp: 416.0000 - precision: 0.2783 - recall: 0.9976 - val_loss: 0.0122 - val_fn: 13.0000 - val_fp: 160.0000 - val_tn: 56726.0000 - val_tp: 62.0000 - val_precision: 0.2793 - val_recall: 0.8267\n",
            "Epoch 76/100\n",
            "112/112 - 1s - loss: 5.6480e-07 - fn: 1.0000 - fp: 1699.0000 - tn: 225730.0000 - tp: 416.0000 - precision: 0.1967 - recall: 0.9976 - val_loss: 0.0428 - val_fn: 13.0000 - val_fp: 252.0000 - val_tn: 56634.0000 - val_tp: 62.0000 - val_precision: 0.1975 - val_recall: 0.8267\n",
            "Epoch 77/100\n",
            "112/112 - 1s - loss: 2.9189e-07 - fn: 4.0000 - fp: 1762.0000 - tn: 225667.0000 - tp: 413.0000 - precision: 0.1899 - recall: 0.9904 - val_loss: 0.0241 - val_fn: 8.0000 - val_fp: 247.0000 - val_tn: 56639.0000 - val_tp: 67.0000 - val_precision: 0.2134 - val_recall: 0.8933\n",
            "Epoch 78/100\n",
            "112/112 - 1s - loss: 4.6545e-07 - fn: 4.0000 - fp: 2571.0000 - tn: 224858.0000 - tp: 413.0000 - precision: 0.1384 - recall: 0.9904 - val_loss: 0.0552 - val_fn: 13.0000 - val_fp: 146.0000 - val_tn: 56740.0000 - val_tp: 62.0000 - val_precision: 0.2981 - val_recall: 0.8267\n",
            "Epoch 79/100\n",
            "112/112 - 1s - loss: 3.5533e-07 - fn: 1.0000 - fp: 1618.0000 - tn: 225811.0000 - tp: 416.0000 - precision: 0.2045 - recall: 0.9976 - val_loss: 0.0364 - val_fn: 12.0000 - val_fp: 493.0000 - val_tn: 56393.0000 - val_tp: 63.0000 - val_precision: 0.1133 - val_recall: 0.8400\n",
            "Epoch 80/100\n",
            "112/112 - 1s - loss: 5.0262e-07 - fn: 4.0000 - fp: 2671.0000 - tn: 224758.0000 - tp: 413.0000 - precision: 0.1339 - recall: 0.9904 - val_loss: 0.0180 - val_fn: 12.0000 - val_fp: 170.0000 - val_tn: 56716.0000 - val_tp: 63.0000 - val_precision: 0.2704 - val_recall: 0.8400\n",
            "Epoch 81/100\n",
            "112/112 - 1s - loss: 2.1477e-07 - fn: 2.0000 - fp: 2094.0000 - tn: 225335.0000 - tp: 415.0000 - precision: 0.1654 - recall: 0.9952 - val_loss: 0.0455 - val_fn: 9.0000 - val_fp: 682.0000 - val_tn: 56204.0000 - val_tp: 66.0000 - val_precision: 0.0882 - val_recall: 0.8800\n",
            "Epoch 82/100\n",
            "112/112 - 1s - loss: 1.4935e-07 - fn: 2.0000 - fp: 1676.0000 - tn: 225753.0000 - tp: 415.0000 - precision: 0.1985 - recall: 0.9952 - val_loss: 0.0210 - val_fn: 11.0000 - val_fp: 347.0000 - val_tn: 56539.0000 - val_tp: 64.0000 - val_precision: 0.1557 - val_recall: 0.8533\n",
            "Epoch 83/100\n",
            "112/112 - 1s - loss: 1.3005e-07 - fn: 1.0000 - fp: 1311.0000 - tn: 226118.0000 - tp: 416.0000 - precision: 0.2409 - recall: 0.9976 - val_loss: 0.0124 - val_fn: 14.0000 - val_fp: 155.0000 - val_tn: 56731.0000 - val_tp: 61.0000 - val_precision: 0.2824 - val_recall: 0.8133\n",
            "Epoch 84/100\n",
            "112/112 - 1s - loss: 6.9410e-07 - fn: 2.0000 - fp: 2278.0000 - tn: 225151.0000 - tp: 415.0000 - precision: 0.1541 - recall: 0.9952 - val_loss: 0.0073 - val_fn: 17.0000 - val_fp: 48.0000 - val_tn: 56838.0000 - val_tp: 58.0000 - val_precision: 0.5472 - val_recall: 0.7733\n",
            "Epoch 85/100\n",
            "112/112 - 1s - loss: 3.7131e-06 - fn: 14.0000 - fp: 3149.0000 - tn: 224280.0000 - tp: 403.0000 - precision: 0.1135 - recall: 0.9664 - val_loss: 0.1259 - val_fn: 11.0000 - val_fp: 237.0000 - val_tn: 56649.0000 - val_tp: 64.0000 - val_precision: 0.2126 - val_recall: 0.8533\n",
            "Epoch 86/100\n",
            "112/112 - 1s - loss: 3.2133e-06 - fn: 9.0000 - fp: 4137.0000 - tn: 223292.0000 - tp: 408.0000 - precision: 0.0898 - recall: 0.9784 - val_loss: 0.2908 - val_fn: 13.0000 - val_fp: 296.0000 - val_tn: 56590.0000 - val_tp: 62.0000 - val_precision: 0.1732 - val_recall: 0.8267\n",
            "Epoch 87/100\n",
            "112/112 - 1s - loss: 2.7599e-06 - fn: 11.0000 - fp: 6043.0000 - tn: 221386.0000 - tp: 406.0000 - precision: 0.0630 - recall: 0.9736 - val_loss: 0.1436 - val_fn: 10.0000 - val_fp: 468.0000 - val_tn: 56418.0000 - val_tp: 65.0000 - val_precision: 0.1220 - val_recall: 0.8667\n",
            "Epoch 88/100\n",
            "112/112 - 1s - loss: 1.0424e-06 - fn: 7.0000 - fp: 3924.0000 - tn: 223505.0000 - tp: 410.0000 - precision: 0.0946 - recall: 0.9832 - val_loss: 0.0656 - val_fn: 9.0000 - val_fp: 490.0000 - val_tn: 56396.0000 - val_tp: 66.0000 - val_precision: 0.1187 - val_recall: 0.8800\n",
            "Epoch 89/100\n",
            "112/112 - 1s - loss: 4.4026e-06 - fn: 7.0000 - fp: 3939.0000 - tn: 223490.0000 - tp: 410.0000 - precision: 0.0943 - recall: 0.9832 - val_loss: 0.0448 - val_fn: 14.0000 - val_fp: 138.0000 - val_tn: 56748.0000 - val_tp: 61.0000 - val_precision: 0.3065 - val_recall: 0.8133\n",
            "Epoch 90/100\n",
            "112/112 - 1s - loss: 7.5071e-07 - fn: 3.0000 - fp: 2892.0000 - tn: 224537.0000 - tp: 414.0000 - precision: 0.1252 - recall: 0.9928 - val_loss: 0.0209 - val_fn: 13.0000 - val_fp: 180.0000 - val_tn: 56706.0000 - val_tp: 62.0000 - val_precision: 0.2562 - val_recall: 0.8267\n",
            "Epoch 91/100\n",
            "112/112 - 1s - loss: 3.3453e-06 - fn: 5.0000 - fp: 2705.0000 - tn: 224724.0000 - tp: 412.0000 - precision: 0.1322 - recall: 0.9880 - val_loss: 0.2252 - val_fn: 11.0000 - val_fp: 404.0000 - val_tn: 56482.0000 - val_tp: 64.0000 - val_precision: 0.1368 - val_recall: 0.8533\n",
            "Epoch 92/100\n",
            "112/112 - 1s - loss: 1.2017e-06 - fn: 6.0000 - fp: 3219.0000 - tn: 224210.0000 - tp: 411.0000 - precision: 0.1132 - recall: 0.9856 - val_loss: 0.0393 - val_fn: 12.0000 - val_fp: 275.0000 - val_tn: 56611.0000 - val_tp: 63.0000 - val_precision: 0.1864 - val_recall: 0.8400\n",
            "Epoch 93/100\n",
            "112/112 - 1s - loss: 5.8904e-07 - fn: 3.0000 - fp: 3639.0000 - tn: 223790.0000 - tp: 414.0000 - precision: 0.1021 - recall: 0.9928 - val_loss: 0.0184 - val_fn: 15.0000 - val_fp: 146.0000 - val_tn: 56740.0000 - val_tp: 60.0000 - val_precision: 0.2913 - val_recall: 0.8000\n",
            "Epoch 94/100\n",
            "112/112 - 1s - loss: 3.8315e-07 - fn: 3.0000 - fp: 1836.0000 - tn: 225593.0000 - tp: 414.0000 - precision: 0.1840 - recall: 0.9928 - val_loss: 0.0248 - val_fn: 11.0000 - val_fp: 193.0000 - val_tn: 56693.0000 - val_tp: 64.0000 - val_precision: 0.2490 - val_recall: 0.8533\n",
            "Epoch 95/100\n",
            "112/112 - 1s - loss: 7.4744e-07 - fn: 3.0000 - fp: 1423.0000 - tn: 226006.0000 - tp: 414.0000 - precision: 0.2254 - recall: 0.9928 - val_loss: 0.0777 - val_fn: 10.0000 - val_fp: 551.0000 - val_tn: 56335.0000 - val_tp: 65.0000 - val_precision: 0.1055 - val_recall: 0.8667\n",
            "Epoch 96/100\n",
            "112/112 - 1s - loss: 4.4381e-07 - fn: 2.0000 - fp: 2169.0000 - tn: 225260.0000 - tp: 415.0000 - precision: 0.1606 - recall: 0.9952 - val_loss: 0.0372 - val_fn: 11.0000 - val_fp: 262.0000 - val_tn: 56624.0000 - val_tp: 64.0000 - val_precision: 0.1963 - val_recall: 0.8533\n",
            "Epoch 97/100\n",
            "112/112 - 1s - loss: 2.5743e-07 - fn: 0.0000e+00 - fp: 1830.0000 - tn: 225599.0000 - tp: 417.0000 - precision: 0.1856 - recall: 1.0000 - val_loss: 0.0155 - val_fn: 14.0000 - val_fp: 95.0000 - val_tn: 56791.0000 - val_tp: 61.0000 - val_precision: 0.3910 - val_recall: 0.8133\n",
            "Epoch 98/100\n",
            "112/112 - 1s - loss: 1.4371e-07 - fn: 0.0000e+00 - fp: 842.0000 - tn: 226587.0000 - tp: 417.0000 - precision: 0.3312 - recall: 1.0000 - val_loss: 0.0157 - val_fn: 13.0000 - val_fp: 148.0000 - val_tn: 56738.0000 - val_tp: 62.0000 - val_precision: 0.2952 - val_recall: 0.8267\n",
            "Epoch 99/100\n",
            "112/112 - 1s - loss: 1.5209e-07 - fn: 3.0000 - fp: 943.0000 - tn: 226486.0000 - tp: 414.0000 - precision: 0.3051 - recall: 0.9928 - val_loss: 0.0128 - val_fn: 14.0000 - val_fp: 133.0000 - val_tn: 56753.0000 - val_tp: 61.0000 - val_precision: 0.3144 - val_recall: 0.8133\n",
            "Epoch 100/100\n",
            "112/112 - 1s - loss: 1.8619e-07 - fn: 1.0000 - fp: 1600.0000 - tn: 225829.0000 - tp: 416.0000 - precision: 0.2063 - recall: 0.9976 - val_loss: 0.0134 - val_fn: 12.0000 - val_fp: 141.0000 - val_tn: 56745.0000 - val_tp: 63.0000 - val_precision: 0.3088 - val_recall: 0.8400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkMr3Ek7vNLe"
      },
      "source": [
        "F_score = (0.3088 + 0.8400)/ 2"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHkHbiRlvhF2",
        "outputId": "fe900e9b-629f-4599-e040-42ceecdf74c7"
      },
      "source": [
        "F_score"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5744"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6hj12M3ytQW"
      },
      "source": [
        "Conclusions\n",
        "At the end of training, out of 56,961 validation transactions, we are:\n",
        "\n",
        "Correctly identifying 63 of them as fraudulent\n",
        "Missing 12 fraudulent transactions\n",
        "At the cost of incorrectly flagging 141 legitimate transactions\n",
        "In the real world, one would put an even higher weight on class 1, so as to reflect that False Negatives are more costly than False Positives.\n",
        "\n",
        "Next time your credit card gets declined in an online purchase -- this is why"
      ]
    }
  ]
}